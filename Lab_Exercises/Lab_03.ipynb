{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f65a3940",
   "metadata": {},
   "source": [
    "# LAB Sheet-03: Prepare Your Data For Machine Learning\n",
    "\n",
    "### Submitted by : Vatsalya Gupta, SC19B098, B.Tech. ECE VII Semester"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0955c7",
   "metadata": {},
   "source": [
    "Many machine learning algorithms make assumptions about your data. It is often a very good\n",
    "idea to prepare your data in such way to best expose the structure of the problem to the machine\n",
    "learning algorithms that you intend to use. In this LAB Sheet you will discover how to prepare\n",
    "your data for machine learning in Python using scikit-learn. After completing this lesson you\n",
    "will know how to:\n",
    "1. Rescale data.\n",
    "2. Standardize data.\n",
    "3. Normalize data.\n",
    "4. Binarize data.\n",
    "\n",
    "Letâ€™s get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3417966e",
   "metadata": {},
   "source": [
    "##  Need For Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419d74bc",
   "metadata": {},
   "source": [
    "You almost always need to pre-process your data. It is a required step. A difficulty is that\n",
    "different algorithms make different assumptions about your data and may require different\n",
    "transforms. Further, when you follow all of the rules and prepare your data, sometimes algorithms\n",
    "can deliver better results without pre-processing.\n",
    "Generally, it is recommend creating many different views and transforms of your data,\n",
    "then exercise a handful of algorithms on each view of your dataset. This will help you to flush\n",
    "out which data transforms might be better at exposing the structure of your problem in general."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf766fc",
   "metadata": {},
   "source": [
    "## Data Transforms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6a06a4",
   "metadata": {},
   "source": [
    "In this section you will work through 4 different data pre-processing recipes for machine learning.\n",
    "The Pima Indian diabetes dataset is used in each recipe. Each recipe follows the same structure:\n",
    "- Load the dataset.\n",
    "- Split the dataset into the input and output variables for machine learning.\n",
    "- Apply a pre-processing transform to the input variables.\n",
    "- Summarize the data to show the change.\n",
    "\n",
    "The scikit-learn library provides two standard idioms for transforming data. Each are useful\n",
    "in different circumstances. The transforms are calculated in such a way that they can be applied\n",
    "to your training data and any samples of data you may have in the future. The scikit-learn\n",
    "documentation has some information on how to use various different pre-processing methods:\n",
    "- Fit and Multiple Transform.\n",
    "- Combined Fit-And-Transform.\n",
    "\n",
    "The Fit and Multiple Transform method is the preferred approach. You call the $\\texttt{fit()}$\n",
    "function to prepare the parameters of the transform once on your data. Then later you can use\n",
    "the $\\texttt{transform()}$ function on the same data to prepare it for modeling and again on the test or\n",
    "validation dataset or new data that you may see in the future. The Combined Fit-And-Transform\n",
    "is a convenience that you can use for one off tasks. This might be useful if you are interested\n",
    "in plotting or summarizing the transformed data.\n",
    "You can review the [$\\texttt{preprocess}$ API in\n",
    "scikit-learn.](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8664459",
   "metadata": {},
   "source": [
    "## <font color = 'brown'>1. Rescale Data</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb75391",
   "metadata": {},
   "source": [
    "When your data is comprised of attributes with varying scales, many machine learning algorithms\n",
    "can benefit from rescaling the attributes to all have the same scale. Often this is referred to\n",
    "as normalization and attributes are often rescaled into the range between 0 and 1. This is\n",
    "useful for optimization algorithms used in the core of machine learning algorithms like gradient\n",
    "descent. It is also useful for algorithms that weight inputs like regression and neural networks\n",
    "and algorithms that use distance measures like k-Nearest Neighbors. You can rescale your data\n",
    "using scikit-learn using the [$\\texttt{MinMaxScaler}$ class](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f3e8cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\miniconda3\\envs\\prml\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.353 0.744 0.59  0.354 0.    0.501 0.234 0.483]\n",
      " [0.059 0.427 0.541 0.293 0.    0.396 0.117 0.167]\n",
      " [0.471 0.92  0.525 0.    0.    0.347 0.254 0.183]\n",
      " [0.059 0.447 0.541 0.232 0.111 0.419 0.038 0.   ]\n",
      " [0.    0.688 0.328 0.354 0.199 0.642 0.944 0.2  ]]\n"
     ]
    }
   ],
   "source": [
    "# Rescale data (between 0 and 1)\n",
    "from pandas import read_csv\n",
    "from numpy import set_printoptions\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "filename = 'pima-indians-diabetes_data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "# separate array into input and output components\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "rescaledX = scaler.fit_transform(X)\n",
    "# summarize transformed data\n",
    "set_printoptions(precision=3)\n",
    "print(rescaledX[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c04054",
   "metadata": {},
   "source": [
    "## <font color = 'brown'>2. Standardize Data </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12febc1",
   "metadata": {},
   "source": [
    "Standardization is a useful technique to transform attributes with a Gaussian distribution and\n",
    "differing means and standard deviations to a standard Gaussian distribution with a mean of\n",
    "0 and a standard deviation of 1. It is most suitable for techniques that assume a Gaussian\n",
    "distribution in the input variables and work better with rescaled data, such as linear regression,\n",
    "logistic regression and linear discriminant analysis. You can standardize data using scikit-learn\n",
    "with the [$\\texttt{StandardScaler}$ class.](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1159571c",
   "metadata": {},
   "source": [
    "Perform standardization on the data and print the first 5 rows of all the colums of input X (as seperated before) using $\\texttt{StandardScaler}$ class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "316f1dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.64   0.848  0.15   0.907 -0.693  0.204  0.468  1.426]\n",
      " [-0.845 -1.123 -0.161  0.531 -0.693 -0.684 -0.365 -0.191]\n",
      " [ 1.234  1.944 -0.264 -1.288 -0.693 -1.103  0.604 -0.106]\n",
      " [-0.845 -0.998 -0.161  0.155  0.123 -0.494 -0.921 -1.042]\n",
      " [-1.142  0.504 -1.505  0.907  0.766  1.41   5.485 -0.02 ]]\n"
     ]
    }
   ],
   "source": [
    "# Your code solution to Standardize data (0 mean, 1 stdev) here.\n",
    "from pandas import read_csv\n",
    "from numpy import set_printoptions\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "filename = 'pima-indians-diabetes_data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "# separate array into input and output components\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "scaler = StandardScaler()\n",
    "rescaledX = scaler.fit_transform(X)\n",
    "# summarize transformed data\n",
    "set_printoptions(precision=3)\n",
    "print(rescaledX[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa54c2e1",
   "metadata": {},
   "source": [
    "## <font color = 'brown'>3. Normalize Data</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5bcb3e",
   "metadata": {},
   "source": [
    "Normalizing in scikit-learn refers to rescaling each observation (row) to have a length of 1 (called\n",
    "a unit norm or a vector with the length of 1 in linear algebra). This pre-processing method\n",
    "can be useful for sparse datasets (lots of zeros) with attributes of varying scales when using\n",
    "algorithms that weight input values such as neural networks and algorithms that use distance\n",
    "measures such as k-Nearest Neighbors. You can normalize data in Python with scikit-learn\n",
    "using the [$\\texttt{Normalizer}$ class.](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6023c597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.034 0.828 0.403 0.196 0.    0.188 0.004 0.28 ]\n",
      " [0.008 0.716 0.556 0.244 0.    0.224 0.003 0.261]\n",
      " [0.04  0.924 0.323 0.    0.    0.118 0.003 0.162]\n",
      " [0.007 0.588 0.436 0.152 0.622 0.186 0.001 0.139]\n",
      " [0.    0.596 0.174 0.152 0.731 0.188 0.01  0.144]]\n"
     ]
    }
   ],
   "source": [
    "# Normalize data (length of 1)\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from pandas import read_csv\n",
    "from numpy import set_printoptions\n",
    "filename = 'pima-indians-diabetes_data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "# separate array into input and output components\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "scaler = Normalizer().fit(X)\n",
    "normalizedX = scaler.transform(X)\n",
    "# summarize transformed data\n",
    "set_printoptions(precision=3)\n",
    "print(normalizedX[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdebe09",
   "metadata": {},
   "source": [
    "Using the function you defined in LAB Sheet-01 to compute the magnitude of a vector, check the correctness of your normalized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad69e2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.034 0.828 0.403 0.196 0.    0.188 0.004 0.28 ]\n",
      " [0.008 0.716 0.556 0.244 0.    0.224 0.003 0.261]\n",
      " [0.04  0.924 0.323 0.    0.    0.118 0.003 0.162]\n",
      " [0.007 0.588 0.436 0.152 0.622 0.186 0.001 0.139]\n",
      " [0.    0.596 0.174 0.152 0.731 0.188 0.01  0.144]]\n"
     ]
    }
   ],
   "source": [
    "# Your code solution here\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from pandas import read_csv\n",
    "from numpy import set_printoptions, sqrt\n",
    "def Magnitude(arr):\n",
    "    mag = 0\n",
    "    for i in arr:\n",
    "        mag += i**2\n",
    "    mag = sqrt(mag)\n",
    "    return mag\n",
    "filename = 'pima-indians-diabetes_data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "# separate array into input and output components\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "normalizedX = X.copy()\n",
    "for i in range(len(normalizedX)):\n",
    "    mag = Magnitude(normalizedX[i].transpose())\n",
    "    normalizedX[i] /= mag\n",
    "# summarize transformed data\n",
    "set_printoptions(precision=3)\n",
    "print(normalizedX[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94348fb4",
   "metadata": {},
   "source": [
    "## <font color = 'brown'>4. Binarize Data (Make Binary)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263586c6",
   "metadata": {},
   "source": [
    "You can transform your data using a binary threshold. All values above the threshold are\n",
    "marked 1 and all equal to or below are marked as 0. This is called binarizing your data or\n",
    "thresholding your data. It can be useful when you have probabilities that you want to make crisp\n",
    "values. It is also useful when feature engineering and you want to add new features that indicate\n",
    "something meaningful. You can create new binary attributes in Python using scikit-learn with\n",
    "the [$\\texttt{Binarizer}$ class.](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Binarizer.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d8ac2b",
   "metadata": {},
   "source": [
    "Perform binarization on the data and print the first 5 rows of all the colums of input X (as seperated before) using $\\texttt{Binarizer}$ class.\n",
    "Do it for the threshold values $0, 1, \\& 10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a84a372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 1. 0. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 0. 1. 1. 1.]\n",
      " [1. 1. 1. 0. 0. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 1. 1. 1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Your code solution on binarization here.\n",
    "from pandas import read_csv\n",
    "from numpy import set_printoptions\n",
    "from sklearn.preprocessing import Binarizer\n",
    "filename = 'pima-indians-diabetes_data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "# separate array into input and output components\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "scaler = Binarizer()\n",
    "rescaledX = scaler.fit_transform(X)\n",
    "# summarize transformed data\n",
    "set_printoptions(precision=3)\n",
    "print(rescaledX[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81829a9c",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129364c9",
   "metadata": {},
   "source": [
    "In this LAB sheet you discovered how you can prepare your data for machine learning in Python\n",
    "using scikit-learn. You now have recipes to:\n",
    "- Rescale data.\n",
    "- Standardize data.\n",
    "- Normalize data.\n",
    "- Binarize data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
